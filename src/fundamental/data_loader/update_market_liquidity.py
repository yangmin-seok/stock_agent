# crawler.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
from typing import Optional, Dict, Any
import logging
from datetime import datetime
import psycopg2.extras
# ì•„ë˜ ê²½ë¡œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
from src.fundamental.data_loader.db_util import get_db_connection, setup_database
from src.fundamental.data_loader.config import DB_CONFIG
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def scrape_naver_market_liquidity_by_page(page: int) -> Optional[Dict[str, Any]]:
    """
    ë„¤ì´ë²„ ê¸ˆìœµ 'ì¦ì‹œìê¸ˆë™í–¥' í˜ì´ì§€ì—ì„œ ê³ ê°ì˜ˆíƒê¸ˆê³¼ ì‹ ìš©ì”ê³  ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    ì´ í˜ì´ì§€ëŠ” ë³´í†µ ìµœì‹  ê±°ë˜ì¼ì˜ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    base_url = "https://finance.naver.com/sise/sise_deposit.naver"
    url = f"{base_url}?page={page}"  # ì½”ìŠ¤í”¼ ì „ì²´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'
    }
    
    logger.info("ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì¦ì‹œìê¸ˆë™í–¥ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
    all_data_in_page = []
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
     
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë°ì´í„° í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° í–‰(tr)ì„ ì„ íƒ
        # thë¥¼ í¬í•¨í•˜ëŠ” í—¤ë” í–‰(ìƒìœ„ 3ê°œ)ì€ ì œì™¸
        data_rows = soup.select('table.type_1 tr')[3:] # slicing to skip header rows
     
        # ë°ì´í„° í–‰ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ë§ˆì§€ë§‰ í˜ì´ì§€ë¼ëŠ” ì‹ í˜¸)
        if not any(row.find('td', class_='date') for row in data_rows):
            logger.info(f"P.{page}ì—ì„œ ë°ì´í„° í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return []

        for row in data_rows:
            # ë‚ ì§œ tdê°€ ì—†ëŠ” í–‰(êµ¬ë¶„ì„  ë“±)ì€ ê±´ë„ˆë›°ê¸°
            if not row.find('td', class_='date'):
                continue

            cells = row.find_all('td')

            # ë‚ ì§œ í¬ë§·íŒ… (YY.MM.DD -> YYYY-MM-DD)
            date_str = cells[0].text.strip()
            trade_date = datetime.strptime(date_str, '%y.%m.%d').strftime('%Y-%m-%d')
            
            # ê³ ê°ì˜ˆíƒê¸ˆ
            deposits_str = cells[1].text.strip().replace(',', '')
            investor_deposits = int(deposits_str)

            # ì‹ ìš©ì”ê³ 
            credit_str = cells[3].text.strip().replace(',', '')
            credit_balance = int(credit_str)

            # ì‹ ìš©ì”ê³ ìœ¨ ê³„ì‚°
            credit_deposit_ratio = (credit_balance / investor_deposits * 100) if investor_deposits != 0 else 0.0

            result_dict = {
                "trade_date": trade_date,
                "investor_deposits": investor_deposits,
                "credit_balance": credit_balance,
                "credit_deposit_ratio": round(credit_deposit_ratio, 2)
            }
            all_data_in_page.append(result_dict)

        logger.info(f"âœ… P.{page}ì—ì„œ {len(all_data_in_page)}ê±´ì˜ ë°ì´í„° íŒŒì‹± ì„±ê³µ.")
        return all_data_in_page

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ P.{page} ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ P.{page} ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def update_historical_market_liquidity():
    """
    ë„¤ì´ë²„ì—ì„œ ì¦ì‹œ ìœ ë™ì„± ë°ì´í„°ë¥¼ ì²« í˜ì´ì§€ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸš€ ì¦ì‹œ ìœ ë™ì„± ì „ì²´ ë°ì´í„° ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    
    conn = None
    try:
        conn = get_db_connection(DB_CONFIG)
        setup_database(conn, 'src/fundamental/data_loader/sql/market_liquidity_schema.sql')
        logger.info("DB ì—°ê²° ë° í…Œì´ë¸” ì„¤ì • ì™„ë£Œ.")

        page = 1
        while page <= 140:
            # í˜ì´ì§€ë³„ ë°ì´í„° í¬ë¡¤ë§
            daily_data_list = scrape_naver_market_liquidity_by_page(page)
            
            # í¬ë¡¤ë§í•  ë°ì´í„°ê°€ ë” ì´ìƒ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
            if not daily_data_list:
                break
            
            # DB ì €ì¥ ë¡œì§ (UPSERT)
            columns = daily_data_list[0].keys()
            cols_str = ", ".join(f'"{col}"' for col in columns)
            placeholders = ", ".join([f"%({col})s" for col in columns])
            # trade_dateê°€ ì¤‘ë³µë  ê²½ìš° ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì„ ì—…ë°ì´íŠ¸
            update_cols = [col for col in columns if col not in ['trade_date']]
            update_str = ", ".join([f'"{col}" = EXCLUDED."{col}"' for col in update_cols])

            # ON CONFLICT ë¬¸ë²•ìœ¼ë¡œ UPSERT (INSERT or UPDATE) êµ¬í˜„
            sql = f"""
                INSERT INTO market_liquidity ({cols_str}) 
                VALUES ({placeholders}) 
                ON CONFLICT (trade_date) DO UPDATE SET {update_str};
            """
            
            with conn.cursor() as cur:
                psycopg2.extras.execute_batch(cur, sql, daily_data_list)
                conn.commit()
                logger.info(f"ğŸ’¾ P.{page}ì˜ ë°ì´í„° {len(daily_data_list)}ê±´ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥/ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

            page += 1
            time.sleep(1) # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í˜ì´ì§€ ìš”ì²­ ê°„ 1ì´ˆ ëŒ€ê¸°

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if conn:
            conn.close()
            logger.info("DB ì—°ê²° í•´ì œ.")
    
    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    update_historical_market_liquidity()